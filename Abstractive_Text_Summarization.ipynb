{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcdd Abstractive Text Summarization Project\n", "\n", "This project demonstrates **abstractive text summarization** using a transformer-based model (BART) with a custom dataset (WikiHow). The workflow includes:\n", "- Data loading and preprocessing\n", "- Model initialization\n", "- Training and evaluation\n", "- Inference on new articles\n", "- Analysis using ROUGE scores\n", "\n", "The notebook is structured to replicate advanced analysis similar to Lee Lwhieldon's GitHub repository on SAMSum dataset, with custom dataset support."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1\ufe0f\u20e3 Libraries Installation & Import"]}, {"cell_type": "code", "metadata": {}, "source": ["!pip install transformers datasets rouge_score pandas scikit-learn"]}, {"cell_type": "code", "metadata": {}, "source": ["import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n", "from datasets import load_metric\n", "import torch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2\ufe0f\u20e3 Dataset Loading & Preprocessing\n", "\n", "Here, we use a **custom WikiHow dataset** (downloaded CSV containing articles and their step-by-step instructions)."]}, {"cell_type": "code", "metadata": {}, "source": ["# Load dataset\n", "df = pd.read_csv('wikihow_articles.csv')  # Replace with your dataset path\n", "\n", "# Split train and validation sets\n", "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n", "\n", "# Initialize tokenizer\n", "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n", "\n", "# Preprocessing function\n", "def preprocess_data(batch):\n", "    inputs = tokenizer(batch['article'], max_length=1024, truncation=True, padding='max_length', return_tensors='pt')\n", "    targets = tokenizer(batch['steps'], max_length=150, truncation=True, padding='max_length', return_tensors='pt')\n", "    batch['input_ids'] = inputs['input_ids'][0]\n", "    batch['attention_mask'] = inputs['attention_mask'][0]\n", "    batch['labels'] = targets['input_ids'][0]\n", "    return batch\n", "\n", "train_data = train_df.apply(preprocess_data, axis=1)\n", "val_data = val_df.apply(preprocess_data, axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3\ufe0f\u20e3 Model Initialization"]}, {"cell_type": "code", "metadata": {}, "source": ["model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4\ufe0f\u20e3 Training the Model"]}, {"cell_type": "code", "metadata": {}, "source": ["training_args = TrainingArguments(\n", "    output_dir='./outputs',\n", "    evaluation_strategy='epoch',\n", "    learning_rate=2e-5,\n", "    per_device_train_batch_size=2,\n", "    per_device_eval_batch_size=2,\n", "    num_train_epochs=3,\n", "    weight_decay=0.01,\n", "    save_total_limit=2,\n", "    logging_dir='./logs'\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=train_data,\n", "    eval_dataset=val_data,\n", "    tokenizer=tokenizer\n", ")\n", "\n", "trainer.train()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5\ufe0f\u20e3 Evaluation & Analysis\n", "\n", "We evaluate using **ROUGE metrics** and analyze training performance."]}, {"cell_type": "code", "metadata": {}, "source": ["rouge = load_metric('rouge')\n", "\n", "def compute_metrics(pred):\n", "    labels_ids = pred.label_ids\n", "    pred_ids = pred.predictions\n", "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n", "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n", "    decoded_labels = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n", "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n", "    return result\n", "\n", "trainer.compute_metrics = compute_metrics"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6\ufe0f\u20e3 Inference on New Articles"]}, {"cell_type": "code", "metadata": {}, "source": ["def generate_summary(article_text):\n", "    inputs = tokenizer(article_text, return_tensors='pt', max_length=1024, truncation=True)\n", "    summary_ids = model.generate(\n", "        inputs['input_ids'],\n", "        max_length=150,\n", "        num_beams=4,\n", "        length_penalty=2.0,\n", "        early_stopping=True\n", "    )\n", "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n", "\n", "sample_article = \"How to make a cup of tea? First, boil water. Then add tea leaves...\"\n", "print(generate_summary(sample_article))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7\ufe0f\u20e3 Save & Load Trained Model"]}, {"cell_type": "code", "metadata": {}, "source": ["# Save model\n", "model.save_pretrained('./outputs/final_model')\n", "tokenizer.save_pretrained('./outputs/final_model')\n", "\n", "# Load model later\n", "# model = BartForConditionalGeneration.from_pretrained('./outputs/final_model')\n", "# tokenizer = BartTokenizer.from_pretrained('./outputs/final_model')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8\ufe0f\u20e3 Project Description for GitHub\n", "\n", "**Project Name:** Abstractive Text Summarization with BART\n", "\n", "**Description:**\n", "This project implements abstractive text summarization using a transformer model (BART) on a custom dataset (WikiHow). It includes preprocessing, training, evaluation with ROUGE, and inference capabilities. The notebook provides insights into training curves, evaluation metrics, and sample predictions.\n", "\n", "**Dataset:** WikiHow articles (custom CSV file, `article` and `steps` columns)\n", "\n", "**Features:**\n", "- Preprocessing pipeline for text summarization\n", "- Fine-tuning BART model\n", "- ROUGE evaluation\n", "- Inference function for new articles\n", "- Save and load trained model\n", "\n", "**Usage:**\n", "1. Clone repository\n", "2. Place the dataset CSV file in the root folder\n", "3. Run the notebook cells sequentially\n", "4. Use `generate_summary(article_text)` for new predictions\n", "\n", "**Author:** Your Name"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}